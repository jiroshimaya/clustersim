{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import torch\n",
    "import sudachipy.dictionary\n",
    "import sudachipy.tokenizer\n",
    "import pickle\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import bcubed\n",
    "import numpy as np\n",
    "from gensim.corpora.dictionary import Dictionary as GensimDictionary\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラスタリングの類似度を評価する関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Pair based f-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cluster_similarity_pair(correct_cluster, test_cluster):\n",
    "  def get_pair_label(cluster):\n",
    "    labels = []\n",
    "    for i0, v0 in enumerate(cluster):\n",
    "      for i1, v1 in enumerate(cluster):\n",
    "        if i1<=i0: continue\n",
    "        labels.append(v0==v1)\n",
    "    return labels\n",
    "\n",
    "  correct_pairs = get_pair_label(correct_cluster)\n",
    "  test_pairs = get_pair_label(test_cluster)\n",
    "  combined_pairs = [(v0,v1) for v0, v1 in zip(correct_pairs, test_pairs)]\n",
    "\n",
    "  correct_true, correct_false = correct_pairs.count(True), correct_pairs.count(False)\n",
    "  test_true, test_false = test_pairs.count(True), test_pairs.count(False)\n",
    "  true_positive = combined_pairs.count((True, True))\n",
    "  false_positive = combined_pairs.count((False, True))\n",
    "  true_negative = combined_pairs.count((False, False))\n",
    "  false_negative = combined_pairs.count((True, False))\n",
    "\n",
    "  scores = {\n",
    "    \"ct_cf_tt_tf\": (correct_true, correct_false, test_true, test_false)\n",
    "    , \"tp_fp_tn_fn\": (true_positive, false_positive, true_negative, false_negative)\n",
    "    , \"precision\": metrics.precision_score(correct_pairs, test_pairs)\n",
    "    , \"recall\": metrics.recall_score(correct_pairs, test_pairs)\n",
    "    , \"f1\": metrics.f1_score(correct_pairs, test_pairs)\n",
    "    , \"accuracy\": metrics.accuracy_score(correct_pairs, test_pairs)\n",
    "  }\n",
    "  return scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ct_cf_tt_tf': (6, 9, 6, 9),\n",
       " 'tp_fp_tn_fn': (2, 4, 5, 4),\n",
       " 'precision': 0.3333333333333333,\n",
       " 'recall': 0.3333333333333333,\n",
       " 'f1': 0.3333333333333333,\n",
       " 'accuracy': 0.4666666666666667}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_similarity_pair([0,0,0,1,1,1],[1,0,0,0,0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity-Inverse Purity F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考 https://stackoverflow.com/questions/34047540/python-clustering-purity-metric\n",
    "def cluster_similarity_purity(correct_cluster, test_cluster):\n",
    "  # compute contingency matrix (also called confusion matrix)\n",
    "  contingency_matrix = metrics.cluster.contingency_matrix(correct_cluster, test_cluster)\n",
    "  # purity\n",
    "  purity = np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "  inverse_purity = np.sum(np.amax(contingency_matrix, axis=1)) / np.sum(contingency_matrix)\n",
    "\n",
    "  f1 = 2*purity*inverse_purity/(purity+inverse_purity)\n",
    "\n",
    "  score = {\n",
    "    \"precision\": purity\n",
    "    , \"recall\": inverse_purity\n",
    "    , \"f1\": f1\n",
    "  }\n",
    "  return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.6666666666666666,\n",
       " 'recall': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_similarity_purity([0,0,0,1,1,1],[1,0,0,0,0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCubed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_similarity_bcubed(correct_cluster, test_cluster):\n",
    "  def get_bcubed_input(cluster):\n",
    "    labels = {i: {v} for i,v in enumerate(cluster)}\n",
    "    return labels\n",
    "\n",
    "  correct_bcubed_input = get_bcubed_input(correct_cluster)\n",
    "  test_bcubed_input = get_bcubed_input(test_cluster)\n",
    "\n",
    "  precision = bcubed.precision(correct_bcubed_input, test_bcubed_input)\n",
    "  recall = bcubed.recall(correct_bcubed_input, test_bcubed_input)\n",
    "  f1 = bcubed.fscore(precision, recall)\n",
    "  \n",
    "  scores = {\n",
    "    \"precision\": precision\n",
    "    , \"recall\": recall\n",
    "    , \"f1\": f1\n",
    "  }\n",
    "  return scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.5555555555555555,\n",
       " 'recall': 0.6666666666666666,\n",
       " 'f1': 0.606060606060606}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_similarity_bcubed([0,0,0,1,1,1],[1,0,0,0,0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"text/titles.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "sports-watch      900\n",
      "dokujo-tsushin    870\n",
      "it-life-hack      870\n",
      "movie-enter       870\n",
      "smax              870\n",
      "kaden-channel     864\n",
      "peachy            842\n",
      "topic-news        770\n",
      "livedoor-homme    511\n",
      "Name: category, dtype: int64\n",
      "\n",
      "train\n",
      "sports-watch      810\n",
      "smax              783\n",
      "it-life-hack      783\n",
      "dokujo-tsushin    783\n",
      "movie-enter       783\n",
      "kaden-channel     777\n",
      "peachy            758\n",
      "topic-news        693\n",
      "livedoor-homme    460\n",
      "Name: category, dtype: int64\n",
      "\n",
      "test\n",
      "sports-watch      90\n",
      "kaden-channel     87\n",
      "smax              87\n",
      "dokujo-tsushin    87\n",
      "movie-enter       87\n",
      "it-life-hack      87\n",
      "peachy            84\n",
      "topic-news        77\n",
      "livedoor-homme    51\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# テストデータの読み込み\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "# 検証を素早くできるようにテストデータ数を制限\n",
    "train_df, test_df = train_test_split(df, train_size=0.9, random_state = 0, shuffle=True, stratify=df[\"category\"])\n",
    "# indexをリセット\n",
    "train_df, test_df = train_df.reset_index(drop=True), test_df.reset_index(drop=True)\n",
    "\n",
    "print(\"all\")\n",
    "print(df[\"category\"].value_counts())\n",
    "print(\"\")\n",
    "print(\"train\")\n",
    "print(train_df[\"category\"].value_counts())\n",
    "print(\"\")\n",
    "print(\"test\")\n",
    "print(test_df[\"category\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding取得のための関数の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "def ngram_tfidf(texts, *, ngram_range = (3,3)):\n",
    "  vectorizer = TfidfVectorizer(\n",
    "                    analyzer=\"char\"\n",
    "                    , ngram_range=ngram_range\n",
    "                    , max_df=0.9\n",
    "                    , min_df = 5)\n",
    "  return vectorizer.fit_transform(texts)\n",
    "\n",
    "def word_tfidf(texts, *, ngram_range = (1,1)):\n",
    "  tokenizer_obj = sudachipy.dictionary.Dictionary(dict=\"full\").create()\n",
    "  mode = sudachipy.tokenizer.Tokenizer.SplitMode.A\n",
    "  wakachi_texts = [\" \".join([m.surface() for m in tokenizer_obj.tokenize(text, mode)]) for text in texts]\n",
    "  vectorizer = TfidfVectorizer(\n",
    "    analyzer = \"word\"\n",
    "    , ngram_range = ngram_range\n",
    "    , max_df = 0.9\n",
    "    , min_df = 5\n",
    "  )\n",
    "  return vectorizer.fit_transform(wakachi_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext_vector(texts, *, model=None, model_path = \"fasttext/cc.ja.300.bin\"):\n",
    "  ft = model or fasttext.load_model(model_path)\n",
    "  tokenizer_obj = sudachipy.dictionary.Dictionary(dict=\"full\").create()\n",
    "  mode = sudachipy.tokenizer.Tokenizer.SplitMode.A\n",
    "  vectors = []\n",
    "  for text in texts:\n",
    "    tokens = tokenizer_obj.tokenize(text)\n",
    "    words = [token.surface() for token in tokens]\n",
    "    vec = ft.get_word_vector(words[0])\n",
    "    for w in words[1:]:\n",
    "      vec += ft.get_word_vector(w)\n",
    "    mean_vec = vec / len(words)\n",
    "    vectors.append(mean_vec)\n",
    "  return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考 https://qiita.com/sonoisa/items/1df94d0a98cd4f209051\n",
    "class SentenceBertJapanese:\n",
    "    def __init__(self, model_name_or_path, device=None):\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "    def encode(self, sentences, batch_size=8):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\", \n",
    "                                           truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        # return torch.stack(all_embeddings).numpy()\n",
    "        return torch.stack(all_embeddings)\n",
    "\n",
    "def sentencebert(texts, *, model=None):\n",
    "    MODEL_NAME = \"sonoisa/sentence-bert-base-ja-mean-tokens-v2\"  # <- v2です。\n",
    "    model = model or SentenceBertJapanese(MODEL_NAME)\n",
    "    sentence_embeddings = model.encode(texts, batch_size=8)\n",
    "    return sentence_embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考：近江崇宏; 金田健太郎; 森長誠; 江間見亜利. BERTによる自然言語処理入門 ―Transformersを使った実践プログラミング― (p.169). Kindle 版. \n",
    "#トークナイザとモデルのロード \n",
    "# 参考 https://qiita.com/sonoisa/items/1df94d0a98cd4f209051\n",
    "class BertJapanese:\n",
    "    def __init__(self, model_name_or_path, *, device=None, model = None):\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = model or BertModel.from_pretrained(model_name_or_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output.last_hidden_state \n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def encode(self, sentences, *, batch_size=8, max_length = 256):\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        all_embeddings = []\n",
    "\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(\n",
    "                        batch, \n",
    "                        max_length=max_length, \n",
    "                        padding='max_length', \n",
    "                        truncation=True, \n",
    "                        return_tensors='pt'\n",
    "                ).to(self.device)\n",
    "            \n",
    "            # 文章ベクトルを計算\n",
    "            # BERTの最終層の出力を平均を計算する。（ただし、[PAD]は除く。）\n",
    "            with torch.no_grad():\n",
    "                model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "                \n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "        return torch.stack(all_embeddings)\n",
    "\n",
    "def get_bert_embeddings(texts, *, model=None):\n",
    "    MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "    model = BertJapanese(MODEL_NAME, model=model)\n",
    "    sentence_embeddings = model.encode(texts, batch_size=8)\n",
    "    return sentence_embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_bert_embeddings(test_df[\"title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embedding/bert_embedding.pickle\",\"wb\") as f:\n",
    "  pickle.dump(torch.stack(embeddings).detach().numpy(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering用の関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-meansでクラスタ分析。とりあえず9つのグループに分けてみる\n",
    "def kmeans_clustering(vectors, *, n_clusters=9):\n",
    "  km_model = KMeans(n_clusters=n_clusters, random_state = 0)\n",
    "  km_model.fit(vectors)\n",
    "  return km_model.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 教師あり（トリグラム、ナイーブベイズ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考 \n",
    "def supervised_naivebayse_vector(texts, *, model=None, train_text = train_df[\"title\"], train_category=train_df[\"category\"]):\n",
    "  if model is None:\n",
    "    model = make_pipeline(\n",
    "      TfidfVectorizer(\n",
    "                      analyzer=\"char\"\n",
    "                      , ngram_range=(3,3)\n",
    "                      , max_df=0.9\n",
    "                      , min_df = 5)\n",
    "      , MultinomialNB()\n",
    "    )\n",
    "    model.fit(train_text, train_category)\n",
    "  return model.predict(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考 https://zenn.dev/robes/articles/424cb97503d16e\n",
    "def get_lda_label(texts, *, train_texts = train_df[\"title\"], num_topics=9):\n",
    "  tokenizer_obj = sudachipy.dictionary.Dictionary(dict=\"full\").create()\n",
    "  mode = sudachipy.tokenizer.Tokenizer.SplitMode.A\n",
    "  def wakachi(text):\n",
    "    return [m.surface() for m in tokenizer_obj.tokenize(text, mode)]\n",
    "\n",
    "  wakachi_texts = [wakachi(text) for text in texts]\n",
    "  # モデル作成\n",
    "  dictionary = GensimDictionary(wakachi_texts)\n",
    "  corpus = [dictionary.doc2bow(text) for text in wakachi_texts]\n",
    "  lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "  # maxを取り出す\n",
    "  topics = []\n",
    "  for doc in corpus:\n",
    "    topic, prob = max(lda[doc], key=lambda x:x[1])\n",
    "    topics.append(topic)\n",
    "  return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## guided lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram tfidf, kmeans\n",
    "X = ngram_tfidf(test_df[\"title\"])\n",
    "test_labels = kmeans_clustering(X)\n",
    "test_df[\"pred_trigram_tfidf_kmeans\"] = test_labels\n",
    "\n",
    "# word tfidf, kmeans\n",
    "X = word_tfidf(test_df[\"title\"])\n",
    "test_labels = kmeans_clustering(X)\n",
    "test_df[\"pred_word_tfidf_kmeans\"] = test_labels\n",
    "\n",
    "# fasttext, kmeans\n",
    "# モデルは以下からダウンロード\n",
    "# https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "#ft = fasttext.load_model('fasttext/cc.ja.300.bin')\n",
    "vectors = fasttext_vector(test_df[\"title\"], model = ft)\n",
    "test_labels = kmeans_clustering(vectors)\n",
    "test_df[\"pred_fasttext_kmeans\"] = test_labels\n",
    "\n",
    "# sentence bert, kmeans\n",
    "embedding_binary_path = \"embedding/sentencebert_embedding.pickle\"\n",
    "if Path(embedding_binary_path).exists():\n",
    "  with open(embedding_binary_path, \"rb\") as f:\n",
    "    sentence_embeddings = pickle.load(f)\n",
    "else:\n",
    "  sentence_embeddings = sentencebert(test_df[\"title\"])\n",
    "  Path(embedding_binary_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "  with open(\"embedding/sentencebert_embedding.pickle\", \"wb\") as f:\n",
    "    pickle.dump(sentence_embeddings.detach().numpy(), f)\n",
    "\n",
    "test_labels = kmeans_clustering(sentence_embeddings)\n",
    "test_df[\"pred_sentencebert_kmeans\"] = test_labels\n",
    "\n",
    "# bert, kmeans\n",
    "embedding_binary_path = \"embedding/bert_embedding.pickle\"\n",
    "if Path(embedding_binary_path).exists():\n",
    "  with open(embedding_binary_path, \"rb\") as f:\n",
    "    sentence_embeddings = pickle.load(f)\n",
    "else:\n",
    "  sentence_embeddings = get_bert_embeddings(test_df[\"title\"])\n",
    "  Path(embedding_binary_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "  with open(\"embedding/sentencebert_embedding.pickle\", \"wb\") as f:\n",
    "    pickle.dump(sentence_embeddings.detach().numpy(), f)\n",
    "\n",
    "test_labels = kmeans_clustering(sentence_embeddings)\n",
    "test_df[\"pred_bert_kmeans\"] = test_labels\n",
    "\n",
    "test_labels = get_lda_label(test_df[\"title\"])\n",
    "test_df[\"pred_lda\"] = test_labels\n",
    "\n",
    "test_labels = supervised_naivebayse_vector(test_df[\"title\"])\n",
    "test_df[\"pred_supervised_naivebayse\"] = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "OUT_DIR = \"prediction\"\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "test_df.to_csv(str(Path(OUT_DIR).joinpath(\"livedoor.csv\")), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "  \"pred_trigram_tfidf_kmeans\"\n",
    "  , \"pred_word_tfidf_kmeans\"\n",
    "  , \"pred_fasttext_kmeans\"\n",
    "  , \"pred_sentencebert_kmeans\"\n",
    "  , \"pred_bert_kmeans\"\n",
    "  , \"pred_lda\"\n",
    "  , \"pred_supervised_naivebayse\"\n",
    "]\n",
    "test_df = pd.read_csv(\"prediction/livedoor.csv\")\n",
    "\n",
    "def display_similarity(cluster_similarity_func, *, columns = columns, correct_label = test_df[\"category\"], test_labels = test_df):\n",
    "  for col in columns:\n",
    "    print(col)\n",
    "    print(cluster_similarity_func(correct_label, test_labels[col]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_trigram_tfidf_kmeans\n",
      "{'ct_cf_tt_tf': (30397, 240819, 172013, 99203), 'tp_fp_tn_fn': (21785, 150228, 90591, 8612), 'precision': 0.12664740455663234, 'recall': 0.7166825673586209, 'f1': 0.21525616323304184, 'accuracy': 0.4143413367942894}\n",
      "\n",
      "pred_word_tfidf_kmeans\n",
      "{'ct_cf_tt_tf': (30397, 240819, 85461, 185755), 'tp_fp_tn_fn': (12802, 72659, 168160, 17595), 'precision': 0.14979932366810592, 'recall': 0.4211599828930487, 'f1': 0.22099466588409952, 'accuracy': 0.667224647513421}\n",
      "\n",
      "pred_fasttext_kmeans\n",
      "{'ct_cf_tt_tf': (30397, 240819, 33096, 238120), 'tp_fp_tn_fn': (6382, 26714, 214105, 24015), 'precision': 0.19283297075175249, 'recall': 0.20995492976280555, 'f1': 0.2010300348069866, 'accuracy': 0.812957200165182}\n",
      "\n",
      "pred_sentencebert_kmeans\n",
      "{'ct_cf_tt_tf': (30397, 240819, 32532, 238684), 'tp_fp_tn_fn': (8420, 24112, 216707, 21977), 'precision': 0.25882208287224884, 'recall': 0.277001019837484, 'f1': 0.2676031718285687, 'accuracy': 0.8300653353784437}\n",
      "\n",
      "pred_bert_kmeans\n",
      "{'ct_cf_tt_tf': (30397, 240819, 35909, 235307), 'tp_fp_tn_fn': (11057, 24852, 215967, 19340), 'precision': 0.30791723523350695, 'recall': 0.36375300194098104, 'f1': 0.33351431243024765, 'accuracy': 0.837059760486107}\n",
      "\n",
      "pred_lda\n",
      "{'ct_cf_tt_tf': (30397, 240819, 35660, 235556), 'tp_fp_tn_fn': (4342, 31318, 209501, 26055), 'precision': 0.12176107683679192, 'recall': 0.14284304372141987, 'f1': 0.13146222202037633, 'accuracy': 0.7884601203468822}\n",
      "\n",
      "pred_supervised_naivebayse\n",
      "{'ct_cf_tt_tf': (30397, 240819, 31432, 239784), 'tp_fp_tn_fn': (19530, 11902, 228917, 10867), 'precision': 0.6213413082209214, 'recall': 0.6424976148962068, 'f1': 0.6317423862588752, 'accuracy': 0.9160484632175093}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_similarity(cluster_similarity_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity-Inverse Purity F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_trigram_tfidf_kmeans\n",
      "{'precision': 0.2903663500678426, 'recall': 0.8208955223880597, 'f1': 0.4289905782443096}\n",
      "\n",
      "pred_word_tfidf_kmeans\n",
      "{'precision': 0.3514246947082768, 'recall': 0.582089552238806, 'f1': 0.43825928497049643}\n",
      "\n",
      "pred_fasttext_kmeans\n",
      "{'precision': 0.3147896879240163, 'recall': 0.34328358208955223, 'f1': 0.32841975688567476}\n",
      "\n",
      "pred_sentencebert_kmeans\n",
      "{'precision': 0.41112618724559025, 'recall': 0.4056987788331072, 'f1': 0.4083944517821643}\n",
      "\n",
      "pred_bert_kmeans\n",
      "{'precision': 0.48846675712347354, 'recall': 0.5183175033921302, 'f1': 0.5029495989788865}\n",
      "\n",
      "pred_lda\n",
      "{'precision': 0.1994572591587517, 'recall': 0.23744911804613297, 'f1': 0.2168013686508171}\n",
      "\n",
      "pred_supervised_naivebayse\n",
      "{'precision': 0.7815468113975577, 'recall': 0.7815468113975577, 'f1': 0.7815468113975577}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_similarity(cluster_similarity_purity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCubed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_trigram_tfidf_kmeans\n",
      "{'precision': 0.7302436789202665, 'recall': 0.2562694948629095, 'f1': 0.3793951945032219}\n",
      "\n",
      "pred_word_tfidf_kmeans\n",
      "{'precision': 0.4351869409102146, 'recall': 0.27031032765763474, 'f1': 0.3334825798222082}\n",
      "\n",
      "pred_fasttext_kmeans\n",
      "{'precision': 0.21664901046033372, 'recall': 0.1995891309097131, 'f1': 0.2077694637396775}\n",
      "\n",
      "pred_sentencebert_kmeans\n",
      "{'precision': 0.2831008772589822, 'recall': 0.27139126072269565, 'f1': 0.2771224287171229}\n",
      "\n",
      "pred_bert_kmeans\n",
      "{'precision': 0.369014582491454, 'recall': 0.3341771760861575, 'f1': 0.350732924859773}\n",
      "\n",
      "pred_lda\n",
      "{'precision': 0.15348895831115014, 'recall': 0.13338026228901453, 'f1': 0.1427298298170722}\n",
      "\n",
      "pred_supervised_naivebayse\n",
      "{'precision': 0.6364946721625434, 'recall': 0.629575059396474, 'f1': 0.6330159564573605}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_similarity(cluster_similarity_bcubed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献／記事"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "文章のベクトル化\n",
    "* [SudachiPy](https://github.com/WorksApplications/SudachiPy/blob/develop/docs/tutorial.md)\n",
    "* [tf-idfでベクトル化したラジオ感想ツイートをクラスタリングして可視化する](https://note.com/himaratsu/n/necefee6e5454)\n",
    "* [機械学習 〜 テキスト分類（ナイーブベイズ分類器） 〜](https://qiita.com/fujin/items/39d450b910bf2be866b5)\n",
    "* [fastTextとDoc2Vecのモデルを作成してニュース記事の多クラス分類の精度を比較する](https://qiita.com/kazuki_hayakawa/items/ca5d4735b9514895e197)\n",
    "* [【日本語モデル付き】2020年に自然言語処理をする人にお勧めしたい文ベクトルモデル](https://qiita.com/sonoisa/items/1df94d0a98cd4f209051)\n",
    "* [https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2)\n",
    "\n",
    "エラー・デバッグ関係\n",
    "* [Pytorch: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead](https://stackoverflow.com/questions/55466298/pytorch-cant-call-numpy-on-variable-that-requires-grad-use-var-detach-num)\n",
    "  * sentence-bertの出力をkmeansに入力したらエラーが出たときの解消方法\n",
    "\n",
    "\n",
    "クラスタリング結果の比較方法\n",
    "* [２つのクラスタリング結果がどのくらい似ているかの指標](https://takemikami.com/2019/02/25/clustdiff.html)\n",
    "* [Precision and recall for clustering?](https://stats.stackexchange.com/questions/15158/precision-and-recall-for-clustering/80194)\n",
    "* [Evaluation of clustering](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee66785c2d12082be357f60b53b6454e1233dc7704302fcc0105c907ee27f10e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
